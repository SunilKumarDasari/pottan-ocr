#!/usr/bin/env bash


datagen(){
  ./gen-glyph-list.js
  python3 ./data_gen.py "$@"
  # ipython3 --matplotlib=tk ./data_gen.py
}

train(){
  python3 train.py "$@"
  # ipython3 --matplotlib=tk train.py -- "$@"
}

extractWikiDump(){
  node ./collect-wiki-data.js "$@"
}


ocr(){
  rm -rf tmp/*
  fullPath="$1"
  shift
  baseName=$(basename "$fullPath")
  withoutExt="${baseName%.*}"
  workingCopy="./tmp/$baseName"
  linesDir="./tmp/$withoutExt"
  mkdir -p ./tmp
  cp "$fullPath" "$workingCopy"
  python ./ocropy/ocropus-gpageseg "$workingCopy" -n
  python3  ./ocr.py "$@"  "$linesDir"/*.png
  rename s/bin.txt/txt/ "$linesDir"/*.txt
  python ./ocropy/ocropus-hocr "$workingCopy" -o ./tmp/out.html
  xdg-open ./tmp/out.html
}



if [ -z $1 ]; then
  cat<<EOF
Usage:
./pottan <command> [ arguments ]

List of available commands ( See '--help' of individual command for more details ):

    datagen         - Prepare training data from data/train.txt & data/validate.txt.

    train           - Run the training

    extractWikiDump - Extract words from wiki xml dump. Output written to stdout. Should be saved as data/<name>.txt

    ocr             - Run charector recognition with a pre-trained model and image file
EOF
else
  cmd=$1
  shift
  $cmd "$@"
fi

