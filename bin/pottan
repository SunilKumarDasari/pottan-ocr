#!/usr/bin/env bash

thisFile=$( readlink -f $0 )
thisDir=$(dirname $thisFile )

export PYTHONPATH+=:$thisDir/../

datagen(){
  # ./gen-glyph-list.js
  python3 -m pottan_ocr.data_gen "$@"
  # ipython3 --matplotlib=tk ./data_gen.py
}

train(){
  python3 -m pottan_ocr.train "$@"
  # ipython3 --matplotlib=tk train.py -- "$@"
}

extractWikiDump(){
  node ./collect-wiki-data.js "$@"
}


ocr(){

  crnnPath="$1"
  fullPath="$2"
  if [[ $1 == '--help' || $1 == '-h' || -z $1 ]];then
cat<<EOF
Usage: ./pottan ocr <trained_model.pth> <iamge_path>
EOF
  exit 0;
  fi

  rm -rf tmp/*
  shift
  baseName=$(basename "$fullPath")
  withoutExt="${baseName%.*}"
  workingCopy="./tmp/$baseName"
  linesDir="./tmp/$withoutExt"
  mkdir -p ./tmp
  cp "$fullPath" "$workingCopy"
  python ./ocropy/ocropus-gpageseg "$workingCopy" -n
  python3  -m pottan_ocr.ocr --crnn "$crnnPath"  "$linesDir"/*.png
  rename s/bin.txt/txt/ "$linesDir"/*.txt
  python ./ocropy/ocropus-hocr "$workingCopy" -o ./tmp/out.html
  xdg-open ./tmp/out.html
}



if [ -z $1 ]; then
  cat<<EOF
Usage:
./pottan <command> [ arguments ]

List of available commands ( See '--help' of individual command for more details ):

    extractWikiDump - Extract words from wiki xml dump ( most most of the text corpus ). Output is written to stdout.

    datagen         - Prepare training data from data/train.txt & data/validate.txt. ( Depreciated. used only for manual varification of training data )

    train           - Run the training

    ocr             - Run charector recognition with a pre-trained model and image file
EOF
else
  cmd=$1
  shift
  $cmd "$@"
fi

